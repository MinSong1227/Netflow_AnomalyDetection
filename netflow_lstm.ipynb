{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876ea48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from datetime import datetime, timedelta\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fe035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Static Data\n",
    "inside_ip = [\n",
    "    \"192.168.10.50\", \"205.174.165.68\", \"192.168.10.51\", \"205.174.165.66\",\"192.168.10.19\", \"192.168.10.17\",\n",
    "    \"192.168.10.16\", \"192.168.10.12\", \"192.168.10.9\", \"192.168.10.5\", \"192.168.10.8\", \"192.168.10.14\", \"192.168.10.15\",\n",
    "    \"192.168.10.25\", \"8.8.8.8\", \"192.168.10.1\"\n",
    "    ]\n",
    "dataset_path = '_'\n",
    "data_col = ['ts', 'te', 'td', 'sa', 'da', 'sp', 'dp', 'pr', 'ipkt', 'ibyt', 'opkt', 'obyt', 'Label']\n",
    "feature_col = ['td', 'ipkt', 'ibyt', 'opkt', 'obyt']\n",
    "device = f'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e838c8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Dataset + ip processing\n",
    "train_tmp = []\n",
    "test_tmp = []\n",
    "ip_set = set(inside_ip)\n",
    "\n",
    "with open(rf\"{dataset_path}\\Monday.csv\", 'r', encoding='utf-8') as f:\n",
    "    for line in tqdm(f.readlines()[1:]):\n",
    "        tmp = line.strip().split(',')\n",
    "        if tmp[3] in ip_set:\n",
    "            train_tmp.append(tmp)\n",
    "        else:\n",
    "            tmp[3], tmp[4] = tmp[4], tmp[3]\n",
    "            tmp[5], tmp[6] = tmp[6], tmp[5]\n",
    "            tmp[8], tmp[10] = tmp[10], tmp[8]\n",
    "            tmp[9], tmp[11] = tmp[11], tmp[9]\n",
    "            train_tmp.append(tmp)\n",
    "            \n",
    "train_dataset  = pd.DataFrame(train_tmp, columns = data_col)\n",
    "\n",
    "test_day = ['Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
    "for day in test_day:\n",
    "    with open(rf\"{dataset_path}\\{day}.csv\", 'r', encoding='utf-8') as f:\n",
    "        for line in tqdm(f.readlines()[1:]):\n",
    "            tmp = line.strip().split(',')\n",
    "            if tmp[3] in ip_set:\n",
    "                test_tmp.append(tmp)\n",
    "            else:\n",
    "                tmp[3], tmp[4] = tmp[4], tmp[3]\n",
    "                tmp[5], tmp[6] = tmp[6], tmp[5]\n",
    "                tmp[8], tmp[10] = tmp[10], tmp[8]\n",
    "                tmp[9], tmp[11] = tmp[11], tmp[9]\n",
    "            test_tmp.append(tmp)\n",
    "test_dataset = pd.DataFrame(test_tmp, columns = data_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f65d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['ts'] = pd.to_datetime(train_dataset['ts'])\n",
    "train_dataset['te'] = pd.to_datetime(train_dataset['te'])\n",
    "test_dataset['ts'] = pd.to_datetime(test_dataset['ts'])\n",
    "test_dataset['te'] = pd.to_datetime(test_dataset['te'])\n",
    "train_dataset = train_dataset.astype({'td':'float', 'ipkt':'float', 'ibyt':'float', 'obyt':'float', 'opkt':'float'})\n",
    "test_dataset = test_dataset.astype({'td':'float', 'ipkt':'float', 'ibyt':'float', 'obyt':'float', 'opkt':'float'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ae274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic_dict = {key : {'MEAN': 0, 'STD' : 0} for key in feature_col}\n",
    "for key in feature_col:\n",
    "    statistic_dict[key]['MEAN'] = np.mean(train_dataset[key])\n",
    "    statistic_dict[key]['STD'] = np.std(train_dataset[key])\n",
    "for key in feature_col:\n",
    "    train_dataset[key] = (train_dataset[key] - statistic_dict[key]['MEAN']) / statistic_dict[key]['STD']\n",
    "    test_dataset[key] = (test_dataset[key] - statistic_dict[key]['MEAN']) / statistic_dict[key]['STD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7d1d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset['index'] = [i for i in range(len(test_dataset))]\n",
    "train_dataset['index'] = [i for i in range(len(train_dataset))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6be5c4",
   "metadata": {},
   "source": [
    "### data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e18da8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(data, timewindow=5, seq_size=10, test=True):\n",
    "    document = []\n",
    "    ret = []\n",
    "    st = data['ts'][0]\n",
    "    et = st + timedelta(minutes=timewindow)\n",
    "    label_list = []\n",
    "    window_size = int((max(data['ts']) - min(data['ts']))/ timedelta(minutes=timewindow)) + 2\n",
    "    for idx in tqdm(range(0,window_size)):\n",
    "        tmp_pd = data[(st <= data['ts']) & (data['ts'] < et)]\n",
    "        ip_list = list(tmp_pd.da.unique())\n",
    "        ip_group = tmp_pd.groupby('da')\n",
    "        for ip in ip_list:\n",
    "            tmp_list = ip_group.get_group(ip)[feature_col].values\n",
    "            tmp_idx = list(ip_group.get_group(ip)[\"index\"].values)\n",
    "            if test:\n",
    "                tmp_label = set(ip_group.get_group(ip)['Label'].unique()) - set(\"BENIGN\")\n",
    "                if len(tmp_label) == 0:\n",
    "                    label = \"BENIGN\"\n",
    "                else:\n",
    "                    label = list(tmp_label)[0]\n",
    "            if len(tmp_list) < seq_size:\n",
    "                tmp_list = np.pad(tmp_list, ((0, seq_size - len(tmp_list)), (0, 0)))\n",
    "                ret.append(tmp_list)\n",
    "                document.append(tmp_idx)\n",
    "                if test:\n",
    "                    label_list.append(label)\n",
    "                \n",
    "            else:\n",
    "                for i in range(len(tmp_list) - seq_size+1):\n",
    "                    ret.append(tmp_list[i:i+seq_size])\n",
    "                    document.append(tmp_idx[i:i+seq_size])\n",
    "                    if test:\n",
    "                        label_list.append(label)\n",
    "        st = et\n",
    "        et = st + timedelta(minutes=timewindow)\n",
    "    if test != True:\n",
    "        label_list = [\"BENIGN\"] * len(ret)\n",
    "    return (ret, label_list, document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23438c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 16\n",
    "batch_size = 16\n",
    "latent = 8\n",
    "hidden_size = 16\n",
    "feature_size = 5\n",
    "timewindow = 10\n",
    "seq_size = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewDataset(Dataset):\n",
    "    def __init__(self, dataset, timewindow, seq_size, test):\n",
    "        super(NewDataset, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.data = []\n",
    "        self.label = []\n",
    "        self.document = []\n",
    "        self.build_data(dataset, test)\n",
    "        \n",
    "    def build_data(self, dataset, test):\n",
    "        self.data, self.label, self.document = build_dataset(dataset, timewindow, seq_size, test)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = torch.FloatTensor(self.data[idx])\n",
    "        label = self.label[idx]\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697d33dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = NewDataset(train_dataset, timewindow, seq_size, False)\n",
    "test_data = NewDataset(test_dataset, timewindow, seq_size, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fc6e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(train_dataset, 'train.pkl')\n",
    "# torch.save(test_dataset, 'test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb2c45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288dd62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model\n",
    "class GRU(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, latent_dim,dropout=0, bidirectional=False):\n",
    "        super(GRU, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_direction = int(bidirectional) + 1\n",
    "\n",
    "        self.gru_enc = nn.GRU(input_size, hidden_size,  dropout=dropout, batch_first=True, num_layers=3)\n",
    "        self.lat_layer = nn.GRU(hidden_size, latent_dim, batch_first=True, dropout=dropout, num_layers=2)\n",
    "        self.gru_dec = nn.GRU(latent_dim, input_size,  batch_first=True, dropout=dropout, num_layers=3)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        \n",
    "        output = input_\n",
    "        \n",
    "        output, _ = self.gru_enc(output)\n",
    "        \n",
    "        en_vec, _ = self.lat_layer(output)\n",
    "        \n",
    "        output, _ = self.gru_dec(en_vec)\n",
    "        \n",
    "        return [output, input_, en_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99973e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GRU(feature_size, hidden_size, latent)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c670692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(range(epoch), desc=\"training\")\n",
    "train_loss = []\n",
    "for e in pbar:\n",
    "    losses = []\n",
    "    for batch in train_dataloader:\n",
    "        data, _ = batch\n",
    "        data = data.to(device)\n",
    "        result = model(data)[0]\n",
    "        loss = criterion(data, result)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        pbar.set_postfix(epoch=f\"{e + 1} of {epoch}\", loss=f\"{losses[-1]:.5f}\")\n",
    "    train_loss.append(sum(losses)/len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c2791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71011f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb218a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "predict_loss = []\n",
    "with torch.inference_mode():\n",
    "    model.eval()\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        data, label = batch\n",
    "        data = data.to(device)\n",
    "        result = model(data)[0]\n",
    "        loss = torch.mean(torch.mean(F.mse_loss(result, data, reduction='none'),dim=1), dim=1)\n",
    "        for i in loss:\n",
    "            predict_loss.append(i)\n",
    "        losses = criterion(data, result)\n",
    "        predict_loss.append(losses)\n",
    "        label_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212c2365",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.mean(train_loss[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afb721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected = []\n",
    "for i in range(len(predict_loss)):\n",
    "    if predict_loss[i] >= threshold:\n",
    "        detected.append((label_list[i][0], predict_loss[i]))\n",
    "    if label_list[i][0] != 'BENIGN':\n",
    "        print(label_list[i][0], predict_loss[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30353489",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = [[] for _ in range(len(test_dataset))]\n",
    "\n",
    "for i in tqdm(range(len(test_data.document))):\n",
    "    idx_list = test_data.document[i]\n",
    "    tmp_loss = predict_loss[i]\n",
    "    for idx in idx_list:\n",
    "        loss_list[idx].append(tmp_loss)\n",
    "        \n",
    "sibal = []\n",
    "\n",
    "for loss_group in tqdm(loss_list):\n",
    "    sibal.append(torch.stack(loss_group, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e566900",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_dict = {\"MAX\":[], \"MIN\":[], \"MEAN\":[]}\n",
    "\n",
    "for i in tqdm(range(len(sibal))):\n",
    "    st_dict[\"MEAN\"].append(torch.mean(sibal[i]).cpu())\n",
    "    st_dict[\"MAX\"].append(torch.max(sibal[i]).cpu())    \n",
    "    st_dict[\"MIN\"].append(torch.min(sibal[i]).cpu())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3632d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset['predict_mean'] = st_dict[\"MEAN\"]\n",
    "test_dataset['predict_min'] = st_dict['MIN']\n",
    "test_dataset['predict_max'] = st_dict[\"MAX\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0149c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = len(test_dataset[(test_dataset[\"Label\"] != \"BENIGN\") & (test_dataset[\"predict_max\"] >= threshold)])\n",
    "fp = len(test_dataset[(test_dataset[\"Label\"] == \"BENIGN\") & (test_dataset[\"predict_max\"] >= threshold)])\n",
    "tn = len(test_dataset[(test_dataset[\"Label\"] == \"BENIGN\") & (test_dataset[\"predict_max\"] < threshold)])\n",
    "fn = len(test_dataset[(test_dataset[\"Label\"] != \"BENIGN\") & (test_dataset[\"predict_max\"] < threshold)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f7ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = tp/(tp+fp)\n",
    "recall = tp/(tp+fn)\n",
    "f1 = 2/((1/precision) + (1/recall))\n",
    "print(precision, recall, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b86be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_df = test_dataset[test_dataset[\"Label\"] != \"BENIGN\"]\n",
    "benign_df = test_dataset[test_dataset[\"Label\"] == \"BENIGN\"]\n",
    "total_df = pd.concat((benign_df, attack_df), ignore_index=True)\n",
    "total_df.loc[total_df[\"Label\"] != 'BENIGN', \"Label\"] = 1\n",
    "total_df.loc[total_df[\"Label\"] == 'BENIGN', \"Label\"] = 0\n",
    "fpr_list, tpr_list, threshold_list = roc_curve(total_df[\"Label\"].astype(int), total_df['predict_max'])\n",
    "print(f\"total,{auc(fpr_list, tpr_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5c2ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f160ae89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
